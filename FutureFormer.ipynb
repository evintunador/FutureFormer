{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562f334c-b16b-42ab-830c-03761b4daf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# used for the tokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf7fba-5e7e-49a9-80e9-1064007a68b4",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "the dataset we'll be using is just TinyShakespeare for sake of simplicity & ability to do run/train locally on any computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd373bf-d326-4939-be18-819bae892805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040a955-06cf-4eb0-a524-3e3c92ba6965",
   "metadata": {},
   "source": [
    "# The Tokenizer\n",
    "\n",
    "We'll be using a very simple tokenizer I trained off of the TinyShakespeare dataset that has 128 total tokens and ignores stuff like special tokens & regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3086ce-a3bf-4a8a-ad3d-ff1730ddb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12]\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "        \n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text)\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ec592-187f-4215-b88a-ec894f35e4e4",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ef54bc-7b2c-49f7-ae86-86d9c05b9590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for minGemma. regular Gemma 2b & 7b are defined below \n",
    "    Explanations for many of these will be more clear later when they actually get used\n",
    "    \"\"\"\n",
    "    # The number of tokens in the vocabulary.\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    \n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256\n",
    "    \n",
    "    # The number of layers in the model.\n",
    "    num_hidden_layers: int = 4\n",
    "    \n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4\n",
    "    \n",
    "    # The number of key-value heads for implementing multi-query attention.\n",
    "    num_key_value_heads: int = 1 \n",
    "    \n",
    "    # The hidden size of the model, AKA the embedding dimension. Each token embedding vector will be this long\n",
    "    hidden_size: int = 128 \n",
    "    \n",
    "    # The inner dimension of the MLP part of the decoder layer\n",
    "    intermediate_size: int = 512\n",
    "    \n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 32\n",
    "    \n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-6 # this is to promote numerical stability & prevent dividing by 0\n",
    "    \n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too. 10,000 is the usual\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # the number of predictions ahead that the FutureFormer can see\n",
    "    jirachi = 2\n",
    "\n",
    "    # whether to print out absolutely everything that happens during inference\n",
    "    verbose = False\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381acc4a-f442-47e2-953c-7f6efb7b5de5",
   "metadata": {},
   "source": [
    "# Rotary Positional Encoding (RoPE)\n",
    "\n",
    "Gemma uses RoPE, which is a popular relative positional encoding scheme. Positional encodings are designed to help the model understand the order of tokens in the text since transformers don't have a built-in ordering to them when reading a sequence. Instead of telling Gemma the precise location of each token (which would be \"absolute\" positional encoding), relative positional encodings only reveal to Gemma the placement of different tokens relative to each other. RoPE is a relative positional encoding scheme that functions by multiplying against query & key matrices in the attention mechanism (rather than, for example, addition along the token embeddings) and uses trigonometric functions to effecitvely \"rotate\" the matrices used in self-attention. RoPE is the standard-to-beat nowadays as it's also find in other notable open-source models like Llama. \n",
    "\n",
    "The essential idea is that any given vector within the query & key matrices has a location $m$ and $n$ respectively. Remember a given vector within a query or key matrix corresponds to a specific token in the sequence. You multiply the rotary matrix corresponding to that position against each the query & key, and then when you further multiply those together you effectively have a comparison that takes into account how far away the two tokens are. \n",
    "\n",
    "If you like math, then you'll see the relation to a kernel. For some arbitrary $ 0 < \\epsilon \\leq \\frac{\\pi}{2N}$ is chosen, where $N$ is the maximum sequence length.\n",
    "\n",
    "$$ \\text{RoPE}(x, m) = x e^{mi\\epsilon} $$\n",
    "$$ \\langle\\text{RoPE}(q_j, m), \\text{RoPE}(k_j, n)\\rangle = \\langle q_je^{mi\\epsilon}, k_je^{ni\\epsilon} \\rangle $$\n",
    "$$ = q_jk_je^{mi\\epsilon}e^{ni\\epsilon} $$\n",
    "$$ = q_jk_je^{(m-n)i\\epsilon} $$\n",
    "$$ = \\text{RoPE}(q_jk_j, m-n) $$\n",
    "\n",
    "To get a more thorough understanding, check out [the original paper](https://arxiv.org/abs/2104.09864), [this slightly more approachable but very thorough guide](https://blog.eleuther.ai/rotary-embeddings/) or [this far more approachable youtube video with visuals of the rotation](https://www.youtube.com/watch?v=o29P0Kpobz0). I'm not going to go super in-depth in my commenting on the code since this topic has been around for like 3 years now and it's not one of my interests and i'm so tired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6384a84e-ca6c-4479-ab8c-4daf3d724b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "    \n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c395f-c4bb-4627-b3d2-a3f6e9a0a365",
   "metadata": {},
   "source": [
    "# Root Mean Square Layer Normalization\n",
    "\n",
    "Gemma uses [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467.pdf) (RMSnorm) which is also not particularly new or interesting. As with other forms of layer normalization, the goal is to keep each layer within the same distribution so as to ensure smooth optimization. \n",
    "\n",
    "The root mean square is a statistic defined by squaring every entry in a given vector, averaging them, and then square-rooting that, which gives us a single floating point value.\n",
    "$$ \\text{RMS}(a) = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^n a_i^2} $$\n",
    "Then we just divide the vector by that value\n",
    "$$ \\bar{a}_i = \\frac{a_i}{\\text{RMS}(a_i)}g_i$$\n",
    "\n",
    "When the mean of summed inputs happens to equal zero, then RMSnorm lines up exactly with the older method [LayerNorm](https://arxiv.org/abs/1607.06450v1). The thesis of RMSnorm is essentially that scaling the variance is the thing that's actually helpful, and centering the mean is not so useful, therefore it makes sense for efficiency's sake to just not set the mean to 0. I'd also like to point out that RMSnorm places the vector onto a hypersphere (a sphere but with many more than 3 dimensions) with radius $\\sqrt{n}$. This doesn't really matter here, but it's important for other projects I plan to release in the coming months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9740b0d-8c3f-4dc1-8fac-450bb3d63e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the RMS Normalization (Root Mean Square Normalization) layer.\n",
    "    RMSNorm is a variant of layer normalization that normalizes the activations\n",
    "    of the previous layer based on their root mean square value.\n",
    "\n",
    "    Parameters:\n",
    "    - dim (int): The dimension of the input features the normalization is applied to.\n",
    "    - eps (float): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "    - add_unit_offset (bool): If True, adds a unit (1) to the learned scaling coefficient, effectively\n",
    "      starting with no scaling. If False, the scaling coefficient starts from zero. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        eps: float = 1e-6,\n",
    "        add_unit_offset: bool = True,\n",
    "    ):\n",
    "        super().__init__() \n",
    "        self.eps = eps  # Small epsilon value for numerical stability since you can't divide by 0\n",
    "        self.add_unit_offset = add_unit_offset  # Flag to determine if a unit should be added to the weight\n",
    "        \n",
    "        # Initialize the weight parameter with zeros, which will be learned during training.\n",
    "        # The shape of the weight is [dim], meaning one weight per feature dimension.\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"\n",
    "        Private helper function to normalize the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The normalized tensor.\n",
    "        \"\"\"\n",
    "        # Calculate the root mean square value for each feature (across the last dimension),\n",
    "        # then use reciprocal square root (rsqrt) for normalization.\n",
    "        # Add self.eps to the denominator for numerical stability.\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the RMSNorm layer.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - output: The normalized and scaled tensor.\n",
    "        \"\"\"\n",
    "        # Normalize the input tensor using the _norm function and ensure the data type matches the input.\n",
    "        x = self._norm(x.float()).type_as(x)\n",
    "        \n",
    "        # If add_unit_offset is True, scale the normalized tensor by (1 + self.weight),\n",
    "        # effectively starting with a scaling factor of 1 (no scaling).\n",
    "        # Otherwise, scale by self.weight only.\n",
    "        if self.add_unit_offset:\n",
    "            output = x * (1 + self.weight)\n",
    "        else:\n",
    "            output = x * self.weight\n",
    "            \n",
    "        # Return the scaled output tensor.\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad660e-0c31-427b-af97-a8f21c40ae02",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "Check out this [recent survey of 400 activation functions](https://arxiv.org/pdf/2402.09092.pdf) or [google's exploration of gated activation functions](https://arxiv.org/pdf/2002.05202.pdf) which introduced GeGLU, which is what we use here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3111446c-52ff-4c36-b441-e6724d6ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a multi-layer perceptron with a GeGLU gating mechanism. The GeGLU\n",
    "    activation combines a standard GeLU activation with a learned gating mechanism, enabling\n",
    "    the network to control the flow of information more dynamically.\n",
    "\n",
    "    Attributes:\n",
    "        gate_proj (nn.Linear): A linear layer that transforms the input tensor to an intermediate\n",
    "                               representation, which is then passed through a GeLU activation for\n",
    "                               gating purposes.\n",
    "        up_proj (nn.Linear): Another linear layer that transforms the input tensor to the same\n",
    "                             intermediate representation but without gating. This representation\n",
    "                             is element-wise multiplied by the gated tensor from `gate_proj`.\n",
    "        down_proj (nn.Linear): A linear layer that transforms the gated and combined tensor back\n",
    "                               to the original dimension of the hidden size, producing the final output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The size of the input and output tensors.\n",
    "            intermediate_size (int): The size of the tensor after the initial transformation\n",
    "                                     and before the gating and final projection. This is typically\n",
    "                                     larger than the hidden size to allow for a richer representation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear transformation for the gating mechanism, projecting input to an intermediate size.\n",
    "        self.gate_proj = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "        # Linear transformation for the input tensor, also projecting to the intermediate size but\n",
    "        # intended for element-wise multiplication with the gated output.\n",
    "        self.up_proj = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "        # Linear transformation to project the gated and combined tensor back to the original\n",
    "        # hidden size, completing the MLP structure.\n",
    "        self.down_proj = nn.Linear(intermediate_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor to the MLP.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        # Applies linear transformation for gating.\n",
    "        gate = self.gate_proj(x)\n",
    "\n",
    "        # Applies GeLU activation to the gate, introducing non-linearity and enabling the gating mechanism.\n",
    "        gate = F.gelu(gate)\n",
    "\n",
    "        # Applies another linear transformation to the input tensor for subsequent combination with the gate.\n",
    "        up = self.up_proj(x)\n",
    "\n",
    "        # Element-wise multiplication of the gated tensor with the transformed input tensor, modulating\n",
    "        # the input based on the gate's activation.\n",
    "        fuse = gate * up\n",
    "\n",
    "        # Applies the final linear transformation to project the modulated tensor back to the hidden size.\n",
    "        outputs = self.down_proj(fuse)\n",
    "\n",
    "        # Returns the final output tensor of the MLP, after gating and modulation.\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e847c83-0fdf-48bd-be37-e225a736531f",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "so in a normal NTP mask we'd just use lower-tril\n",
    "```\n",
    "[[1,0,0,0,0],\n",
    "[1,1,0,0,0],\n",
    "[1,1,1,0,0],\n",
    "[1,1,1,1,0],\n",
    "[1,1,1,1,1]]\n",
    "```\n",
    "but here we want the model to have the option of attending to a couple tokens into the future, but excluding the token that's its job to predict\n",
    "```\n",
    "[[1,0,1,0,0],\n",
    "[1,1,0,1,0],\n",
    "[1,1,1,0,1],\n",
    "[1,1,1,1,0],\n",
    "[1,1,1,1,1]]\n",
    "```\n",
    "how many? for now i've set that as a configurable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d5be96-0100-47b1-ad0b-8c39d4c9193d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 8, 8]),\n",
       " tensor([[[[1., 0., 1., 1., 0., 0., 0., 0.],\n",
       "           [1., 1., 0., 1., 1., 0., 0., 0.],\n",
       "           [1., 1., 1., 0., 1., 1., 0., 0.],\n",
       "           [1., 1., 1., 1., 0., 1., 1., 0.],\n",
       "           [1., 1., 1., 1., 1., 0., 1., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "           [1., 1., 1., 1., 1., 1., 1., 1.]]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_extended_attention_mask(seq_length, future_tokens):\n",
    "    \"\"\" creates our future-sight attention mask \"\"\"\n",
    "    # this is a regular next-token prediction mask that only lets tokens see into the past, not the future\n",
    "    tril = torch.tril(torch.ones((seq_length, seq_length)))\n",
    "    \n",
    "    # Instead of using loops, create a mask for future tokens\n",
    "    # The diagonal of ones will be extended to the right by `future_tokens` positions\n",
    "    if future_tokens > 0:\n",
    "        # the 1's in the top-right of this matrix allow the model to see in the future\n",
    "        future_sight_mask = torch.tril(torch.ones((seq_length, seq_length)), diagonal=future_tokens+1)\n",
    "\n",
    "        # this will be used to mask out the next token specifically\n",
    "        mask_out_next_token = torch.tril(torch.ones((seq_length, seq_length)), diagonal=1)\n",
    "\n",
    "        # further into the future tokens minus the next token plus all the past/present tokens\n",
    "        mask = future_sight_mask -1 * mask_out_next_token + tril\n",
    "    else:\n",
    "        # This function defaults to a regular NTP mask if you don't input anything\n",
    "        mask = tril\n",
    "        \n",
    "    return mask\n",
    "\n",
    "mask = create_extended_attention_mask(8, 2).unsqueeze(0).unsqueeze(0)\n",
    "mask.shape, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181d512a-8fa4-4e54-a511-19e07a9fd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
    "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        \n",
    "        # Ensures that the number of query heads is evenly divisible by the number of KV heads.\n",
    "        assert self.num_heads % self.num_kv_heads == 0\n",
    "        # Determines the number of query heads associated with each KV head.\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.rope_theta\n",
    "\n",
    "        # Calculates the total size for all query projections.\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        # Calculates the total size for all key and value projections.\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "\n",
    "        # Defines the scaling factor for the attention scores.\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        # Initializes the linear projection layer for queries, keys, and values\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, (self.num_heads + 2 * self.num_kv_heads) * self.head_dim, bias=False)\n",
    "        # Initializes the output projection layer, mapping the concatenated attention outputs back to the hidden size.\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "        # for our attention mask we'll use very large negative values to prevent attending to certain tokens\n",
    "        #mask_negatives = torch.full((1, 1, config.max_position_embeddings, config.max_position_embeddings),-2.3819763e38).to(torch.float)\n",
    "        # then we'll replace the lower triangular ones with 0's to allow attention to see past tokens\n",
    "        #mask = torch.triu(mask_negatives, diagonal=1).to(config.device)\n",
    "        # then we'll use `register_buffer` to define self.mask as a tensor that shouldn't undergo gradient descent\n",
    "        #self.register_buffer('mask', mask)\n",
    "    \n",
    "        # let's do a multiplicative next-token prediction attention mask\n",
    "        #mask = torch.triu(torch.ones((1, 1, config.max_position_embeddings, config.max_position_embeddings), device=config.device), diagonal=1)\n",
    "        #mask = 1.0 - mask  # Invert the mask: 1s become 0s (masked), and 0s become 1s (unmasked)\n",
    "        mask = create_extended_attention_mask(config.max_position_embeddings, \n",
    "                                              config.jirachi).unsqueeze(0).unsqueeze(0)\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                # The input tensor to the attention mechanism. shape (batch_size, input_len, hidden_size)\n",
    "                hidden_states: torch.Tensor, \n",
    "                ) -> torch.Tensor:\n",
    "\n",
    "        # Ensures the input tensor is 3-dimensional (batch_size, input_len, hidden_size).\n",
    "        hidden_states_shape = hidden_states.shape\n",
    "        assert len(hidden_states_shape) == 3\n",
    "\n",
    "        # Extracts batch size and input sequence length from the hidden states tensor.\n",
    "        batch_size, input_len, _ = hidden_states_shape\n",
    "\n",
    "        # Applies the linear projection to the hidden state to retrieve our q, k & v projections\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        \n",
    "        # Splits the combined QKV tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],dim=-1)\n",
    "        # for readability's sake it would've made more sense to do these separately; this is just more efficient\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = apply_rotary_emb(xq, self.head_dim, self.theta)\n",
    "        xk = apply_rotary_emb(xk, self.head_dim, self.theta)\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            key = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "            value = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        q = xq.transpose(1, 2)\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        # [batch_size, n_local_heads, input_len, input_len]\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) * self.scaling\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention scores\n",
    "        #scores = scores + self.mask[...,:input_len, :input_len] # make sure mask is the correct size. input_len <= max_seq_len\n",
    "        # is it weird that we're masking with addition of 0's & big negatives instead of multiplication of 1's and 0's or -inf's? \n",
    "        # as far as i'm aware it's weird, although my knowledge could just be out of date\n",
    "        # Let's do multiplicative masking instead\n",
    "        scores = scores * self.mask[...,:input_len, :input_len]\n",
    "\n",
    "        # Applies softmax to the scores to obtain attention probabilities\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.matmul(scores, v)\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        # [batch_size, input_len, hidden_dim]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
    "\n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = self.o_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc735b-8c34-47fb-aea5-6af3a06c616e",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "interestingly, here we normalize not only before or after the decoder block, but also in-between the attention and MLP. Other than that though there's nothing unusual about these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd1fcf1-a1dd-4e6c-8d78-e267398304f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder layer that integrates the GemmaAttention mechanism and multi-layer perceptron (MLP). It includes\n",
    "    normalization steps both before and after the attention mechanism to stabilize and accelerate training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializes the GemmaAttention mechanism with parameters from the config, enabling self-attention within the decoder layer.\n",
    "        self.self_attn = Attention(config)\n",
    "        \n",
    "        # Initializes the GemmaMLP module, providing a non-linear transformation after the attention mechanism.\n",
    "        self.mlp = MLP(\n",
    "            # the hidden dimension of the model\n",
    "            hidden_size = config.hidden_size,\n",
    "            # the number of nodes in the center of the two feedforward layers\n",
    "            intermediate_size = config.intermediate_size,\n",
    "        )\n",
    "        \n",
    "        # Applies RMSNorm normalization to the input of the decoder layer for stable training dynamics.\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size,\n",
    "                                       eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm after the attention mechanism and before the MLP to ensure the output is well-conditioned for further processing.\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n",
    "                                                eps = config.rms_norm_eps)\n",
    "\n",
    "    def forward(self,\n",
    "                # The input tensor to the decoder layer. shape (batch_size, input_len, hidden_size)\n",
    "                hidden_states: torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        # Self Attention Block\n",
    "        # Stores the original input for use as a residual connection, aiding in mitigating the vanishing gradient problem\n",
    "        residual = hidden_states\n",
    "        # Normalizes the input before processing by the attention mechanism.\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        # Processes the normalized input through the GemmaAttention mechanism\n",
    "        hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        # The aforementioned residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # MLP Block\n",
    "        # Again, stores the output of the attention block for use as a residual connection before processing by the MLP.\n",
    "        residual = hidden_states\n",
    "        # Normalizes the output of the attention block before passing it to the MLP, ensuring a stable input distribution.\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        # Transforms the normalized attention output through the MLP, introducing additional non-linearity and capacity to the model.\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # Another residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122f7ab-abb0-4c67-9247-a48f705a7bd9",
   "metadata": {},
   "source": [
    "# The Body of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd3b71d6-23da-44a2-94cc-f57102a9aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Body(nn.Module):\n",
    "    \"\"\" the class that loops through each layer of Gemma \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        # Initialize a normalization layer to be applied after the last decoder layer, stabilizing the output\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self,\n",
    "                # The first residual state of the model. shape (batch_size, input_len, hidden_size)\n",
    "                hidden_states: torch.Tensor\n",
    "               ) -> torch.Tensor:\n",
    "\n",
    "        # Iteratively process the input through each DecoderLayer, passing along necessary parameters for attention.\n",
    "        # The hidden states are updated at each layer, progressively incorporating more complex dependencies and transformations.\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            hidden_states = layer(hidden_states=hidden_states)\n",
    "        \n",
    "        # Apply normalization to the output of the final decoder layer, ensuring the model's output is well-conditioned for subsequent use.\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b76ce1-62d9-4594-8775-4c0eb69562a6",
   "metadata": {},
   "source": [
    "# The Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dea80b7-2d2e-4e0d-9616-6acfee9b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FutureFormer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        config: Config, # the hyperparameters\n",
    "        tokenizer: tokenizer, # the tokenizer. we don't always store the tokenizer inside of the model, but it doesn't matter here\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # the attention heads need to cleanly divide up the hidden_size of the model so that we can split it all apart & combine back together\n",
    "        assert config.hidden_size % config.num_attention_heads == 0\n",
    "\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "         # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(self.vocab_size, config.hidden_size)\n",
    "        \n",
    "        # the body of the model; all the transformer decoder layers\n",
    "        self.model = Body(config)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # the number of time periods into the future the model can look\n",
    "        self.jirachi = config.jirachi\n",
    "\n",
    "        # should we print absolutely everything in the generate function?\n",
    "        self.verbose = config.verbose\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len) list of integer token ids\n",
    "        target_token_ids: torch.Tensor = None, # a shape (batch_size, input_seq_len) list of token ids to train on\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        # turn the input tokens into the first resudial state using the embedding matrix\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size)\n",
    "        hidden_states = self.embedder(input_token_ids)\n",
    "        \n",
    "        # Gemma normalizes the embedding by sqrt(hidden_size)\n",
    "        hidden_states = hidden_states * (self.config.hidden_size**0.5)\n",
    "\n",
    "        # this is where the bulk of the calculations are performed, the actual decoder layers\n",
    "        hidden_states = self.model(hidden_states=hidden_states) # -> (batch_size, input_len, hidden_size)\n",
    "\n",
    "        # grabbing the weights of the embedding matrix shape (vocab_size, hidden_dim) for use as the output layer\n",
    "        embedder_weight = self.embedder.weight\n",
    "\n",
    "        # the embedding matrix is also used as the output layer\n",
    "        # this saves on parameters & makes sense for interpretability\n",
    "        # (batch_size, input_len, hidden_size) @ (hidden_size, vocab_size) -> (batch_size, input_len, vocab_size)\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t())\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            batch_size, input_len, vocab_size = logits.shape\n",
    "            # then we reshape our logits & targets before calculating cross-entropy loss\n",
    "            loss = self.criterion(logits.view(batch_size*input_len, vocab_size), \n",
    "                                  target_token_ids.view(batch_size*input_len))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions from Gemma's output.\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        The class operates as follows:\n",
    "        \n",
    "        1. Computes logits by multiplying the selected hidden states with the transposed embedding matrix. \n",
    "    \n",
    "        2. Temperature is used to scale the logits, making the distribution over tokens sharper (lower temperature) \n",
    "        or flatter (higher temperature), which affects the randomness of the sampling (flatter -> more random)\n",
    "    \n",
    "        3. The softmax function is applied to the scaled logits to obtain a probability distribution over the vocabulary.\n",
    "    \n",
    "        4. For top-p sampling, the function computes the cumulative sum of the sorted probabilities and masks out tokens until the \n",
    "        cumulative probability exceeds the threshold defined by `top_ps`. This allows the model to focus on a subset of the most \n",
    "        probable tokens while ignoring the long tail of less likely tokens. \n",
    "        We to ignore long tail probabilities to avoid nonsensical output\n",
    "    \n",
    "        5. For top-k sampling, the function masks out all tokens except the `k` most likely ones, as specified by `top_ks`. \n",
    "        This ensures that the model only considers a fixed number of the most probable tokens for the next token prediction.\n",
    "    \n",
    "        6. After applying both the top-p and top-k masks, the probabilities are re-normalized so that they sum up to 1\n",
    "    \n",
    "        7. The function then samples from the re-normalized probability distribution to select the next token. \n",
    "        \"\"\"\n",
    "        # Select the last element for each sequence.\n",
    "        # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        #logits = logits[:,-1,:]\n",
    "        # instead we're going to pass in logits that have already been selected\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "        logits.div_(temperature) # div_ is an in-place operation which is ok since we don't record gradients during inference\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k\n",
    "        # both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # probs_sort contains float probabilities while probs_idx contains integer indices\n",
    "\n",
    "        # calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "\n",
    "        # calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        # \"expand\" means copy the original into this new size, so each length vocab_size row is the same\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort,\n",
    "                             dim=-1,\n",
    "                             index=torch.argsort(probs_idx, dim=-1))\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "\n",
    "    def replacement_index(self, n: int):\n",
    "        # calculates the correct index to select from logits or replace from the temporary token sequence\n",
    "        return -1 -(n * (n + 1) // 2)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = 100, # the model will output 100 tokens\n",
    "        temperature: float = 0.95, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
    "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
    "        top_k: int = 65, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "    ) -> str: \n",
    "        \"\"\"Generates responses for given prompts using our FutureFormer algorithm\"\"\"\n",
    "        \n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.max_seq_len\n",
    "\n",
    "        # we'll be keeping track of the actual tokens and a set that will change\n",
    "        tokens_temp = tokens\n",
    "\n",
    "        # the initialization loop to build up our first set of future tokens\n",
    "        if self.verbose: print(\"----------- initialization loop ----------\")\n",
    "        for i in range(self.jirachi):\n",
    "            if self.verbose: print(\"i: \", i)\n",
    "                \n",
    "            for _ in range(i+1):\n",
    "                if self.verbose: print(\"_: \", _)\n",
    "                    \n",
    "                for j in range(i+1):\n",
    "                    if self.verbose: print(\"j: \", j)\n",
    "                        \n",
    "                    # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "                    logits, _ = self(tokens_temp[:,:self.max_seq_len])\n",
    "                    if self.verbose: print(\"logits: \", logits.shape)\n",
    "                    \n",
    "                    # select the timestep of interest\n",
    "                    logits = logits[:, self.replacement_index(j), :]\n",
    "                    if self.verbose: print(\"replacement index j: \", self.replacement_index(j))\n",
    "                    if self.verbose: print(\"logits; \", logits.shape)\n",
    "            \n",
    "                    next_token = self.Sampler(logits = logits,\n",
    "                                              temperature = temperature,\n",
    "                                              top_p = top_p,\n",
    "                                              top_k = top_k)\n",
    "                    if self.verbose: print(\"next_token: \", next_token.shape, next_token)\n",
    "        \n",
    "                    if j == 0:\n",
    "                        # add the token to the sequence\n",
    "                        tokens_temp = torch.cat((tokens_temp[:, -self.max_seq_len:], next_token), dim=1)\n",
    "                        if self.verbose: print(\"tokens_temp: \", tokens_temp.shape, tokens_temp)\n",
    "                    else:\n",
    "                        # replace the previously predicted token\n",
    "                        tokens_temp[:,self.replacement_index(j)] = next_token # originally i\n",
    "                        if self.verbose: print(\"replacement index j: \", self.replacement_index(j)) # originally i\n",
    "                        if self.verbose: print(\"tokens_temp: \", tokens_temp.shape, tokens_temp)\n",
    "\n",
    "        # the more repetitive loop that actually gets our final tokens\n",
    "        if self.verbose: print(\"--------- inference loop ---------\")\n",
    "        for i in range(output_len):\n",
    "            if self.verbose: print(\"i: \", i)\n",
    "                \n",
    "            for j in range(self.jirachi+1):\n",
    "                if self.verbose: print(\"j: \", j)\n",
    "                    \n",
    "                # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "                logits, _ = self(tokens_temp[:,:self.max_seq_len])\n",
    "                if self.verbose: print(\"logits: \", logits.shape)\n",
    "                    \n",
    "                # select the timestep of interest\n",
    "                logits = logits[:, self.replacement_index(j), :]\n",
    "                if self.verbose: print(\"replacement index j: \", self.replacement_index(j))\n",
    "                if self.verbose: print(\"logits: \", logits.shape)\n",
    "            \n",
    "                next_token = self.Sampler(logits = logits,\n",
    "                                          temperature = temperature,\n",
    "                                          top_p = top_p,\n",
    "                                          top_k = top_k)\n",
    "                if self.verbose: print(\"next_token: \", next_token.shape, next_token)\n",
    "                \n",
    "                if j == 0:\n",
    "                    # add the token to the sequence\n",
    "                    tokens_temp = torch.cat((tokens_temp[:, -self.max_seq_len:], next_token), dim=1)\n",
    "                    if self.verbose: print(\"tokens_temp: \", tokens_temp.shape, tokens_temp)\n",
    "                else:\n",
    "                    tokens_temp[:,self.replacement_index(j)] = next_token # originally i\n",
    "                    if self.verbose: print(\"replacement index j: \", self.replacement_index(j)) # originally i\n",
    "                    if self.verbose: print(\"tokens_temp: \", tokens_temp.shape, tokens_temp)\n",
    "\n",
    "                    if j == self.jirachi:  # originally i\n",
    "                        # append the largest model's output to the actual sequence idx\n",
    "                        tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                        if self.verbose: print(\"tokens: \", tokens.shape, tokens)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output\n",
    "\n",
    "        #for i in range(output_len):\n",
    "        #    # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "        #    logits, _ = self(tokens[:,:self.max_seq_len])\n",
    "        #    \n",
    "        #    next_token = self.Sampler(\n",
    "        #        logits = logits, # the actual output of the model\n",
    "        #        temperature = temperature,\n",
    "        #        top_p = top_p,\n",
    "        #        top_k = top_k\n",
    "        #    )\n",
    "#\n",
    "        #    # add our new token to the sequence\n",
    "        #    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "#\n",
    "        ## decode our list of tokens to an actual string\n",
    "        #output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "#\n",
    "        #return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a1f17-5a2e-4bbe-bc99-e9412d2b81de",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee75deb9-dfbc-46dd-93d7-4981369a2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33d02be1-d005-4913-b8b7-630a7b97696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea5b6c7-f555-4f3f-adf5-325d0a608b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[121,   1,  57,  87,   0,  57,  92,  81,   1,  92,  95,   1,  90,   1,\n",
      "          51,  73,  43,   1,  43,  63,  89,   1,  84,   1,  91,  43,   1, 100,\n",
      "          65,  39,  50,   1,  65,  70,   1,  39,   1,  41,  78,   8,   0,  37,\n",
      "          67,   1,  49,  90,  61,   1, 102,  51,   1, 116,  66,  57, 112,  85,\n",
      "          20,  27,  30,  32,  17,  26,  31,  21,  27,  71,  32,  77,  56,  63,\n",
      "          66,  28,  43,  58,  56,  59, 108,  47,  53,  66,  21,   1,  51,  59,\n",
      "          80,   1,  45,  53,   1, 100,  65,   1,  72,  43,  82,  18,  73,   1,\n",
      "          69,   1,  14,  39,  54,  58,  74,  58,  39,   5,  57,   1, 124,  43,\n",
      "          54,   1, 101,   1,  58,  93, 107,  59,  93,   1,  74,  71,  20,  43,\n",
      "           1,  92,  65,   1,  72,   1,  48,  43, 119,  50,   1,  94,   1, 101,\n",
      "           1, 117,  44,  43,   1,  69,   1,  46,  53, 111,  82,  20,  74,   1,\n",
      "          88,  52,  45,  89,  58,   1,  42,  39,  59, 122,  58,  68,  66,  98,\n",
      "          39, 114,  47,  44,  59,  50,   1,  14,  69,  39,  41,  39,  82, 123,\n",
      "           1,  87,  56,   1, 100,  65,  46,  53, 111,  57,   1,  44, 115,  51,\n",
      "           1,  83,   1,  86,   1,  53,  72,  56,   1,  51,  73,  43,  82,  31,\n",
      "          59,  96,  73,  57,   1,  84,   1,  87,  56,   1,  86,   1,  56,  47,\n",
      "          60,  39,  50,  57,   1,  69,   1, 101,   1,  50,  53,  95,  82,  31,\n",
      "          59,  54,  54,  53,  57,  97,   1,  96,   1,  39,   1,  65,  97,   1,\n",
      "          47,  51,  54,  53]])\n",
      "that she\n",
      "shall have no more eyes to see withal than a cat.\n",
      "You know him not, sir.\n",
      "\n",
      "HORTENSIO:\n",
      "Tarry, Petruchio, I must go with thee,\n",
      "For in Baptista's keep my treasure is:\n",
      "He hath the jewel of my life in hold,\n",
      "His youngest daughter, beautiful Binaca,\n",
      "And her withholds from me and other more,\n",
      "Suitors to her and rivals in my love,\n",
      "Supposing it a thing impo\n",
      "-------\n",
      "tensor([[  1,  57,  87,   0,  57,  92,  81,   1,  92,  95,   1,  90,   1,  51,\n",
      "          73,  43,   1,  43,  63,  89,   1,  84,   1,  91,  43,   1, 100,  65,\n",
      "          39,  50,   1,  65,  70,   1,  39,   1,  41,  78,   8,   0,  37,  67,\n",
      "           1,  49,  90,  61,   1, 102,  51,   1, 116,  66,  57, 112,  85,  20,\n",
      "          27,  30,  32,  17,  26,  31,  21,  27,  71,  32,  77,  56,  63,  66,\n",
      "          28,  43,  58,  56,  59, 108,  47,  53,  66,  21,   1,  51,  59,  80,\n",
      "           1,  45,  53,   1, 100,  65,   1,  72,  43,  82,  18,  73,   1,  69,\n",
      "           1,  14,  39,  54,  58,  74,  58,  39,   5,  57,   1, 124,  43,  54,\n",
      "           1, 101,   1,  58,  93, 107,  59,  93,   1,  74,  71,  20,  43,   1,\n",
      "          92,  65,   1,  72,   1,  48,  43, 119,  50,   1,  94,   1, 101,   1,\n",
      "         117,  44,  43,   1,  69,   1,  46,  53, 111,  82,  20,  74,   1,  88,\n",
      "          52,  45,  89,  58,   1,  42,  39,  59, 122,  58,  68,  66,  98,  39,\n",
      "         114,  47,  44,  59,  50,   1,  14,  69,  39,  41,  39,  82, 123,   1,\n",
      "          87,  56,   1, 100,  65,  46,  53, 111,  57,   1,  44, 115,  51,   1,\n",
      "          83,   1,  86,   1,  53,  72,  56,   1,  51,  73,  43,  82,  31,  59,\n",
      "          96,  73,  57,   1,  84,   1,  87,  56,   1,  86,   1,  56,  47,  60,\n",
      "          39,  50,  57,   1,  69,   1, 101,   1,  50,  53,  95,  82,  31,  59,\n",
      "          54,  54,  53,  57,  97,   1,  96,   1,  39,   1,  65,  97,   1,  47,\n",
      "          51,  54,  53,  57]])\n",
      " she\n",
      "shall have no more eyes to see withal than a cat.\n",
      "You know him not, sir.\n",
      "\n",
      "HORTENSIO:\n",
      "Tarry, Petruchio, I must go with thee,\n",
      "For in Baptista's keep my treasure is:\n",
      "He hath the jewel of my life in hold,\n",
      "His youngest daughter, beautiful Binaca,\n",
      "And her withholds from me and other more,\n",
      "Suitors to her and rivals in my love,\n",
      "Supposing it a thing impos\n"
     ]
    }
   ],
   "source": [
    "# a demonstration of what a batch with batch_size=1 looks like. Notice the one-token offset in characters\n",
    "xb, yb = get_batch('train', 1)\n",
    "print(xb)\n",
    "print(tokenizer.decode(xb.squeeze(0).tolist()))\n",
    "print(\"-------\")\n",
    "print(yb)\n",
    "print(tokenizer.decode(yb.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba904bf4-cfbd-4a30-aafe-d76971283b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiating a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9475574f-acff-43a7-ae1b-97ca30f4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.416 K parameters\n",
      "FutureFormer(\n",
      "  (embedder): Embedding(128, 128)\n",
      "  (model): Body(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x Layer(\n",
      "        (self_attn): Attention(\n",
      "          (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FutureFormer(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5242a7-6035-4f53-9d26-0690aea809d9",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16d5b32a-8fc1-4fa2-a301-85142c442bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964.352 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "minGemma(\n",
       "  (embedder): Embedding(65, 128)\n",
       "  (model): GemmaBody(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (qkv_proj): Linear(in_features=128, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (up_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (down_proj): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = minGemma(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/minGemma-vocab_size128-max_position_embeddings256-num_hidden_layers4-num_attention_heads4-num_key_value_heads1-hidden_size128-intermediate_size512-head_dim32-rms_norm_eps1e-06-rope_theta100.0--2024-02-26|11-10-53.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3fd4135-760e-4f67-be11-24bdb497e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 2\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 2\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 129.3206, val loss 129.5448, time elapsed: 0.68 seconds\n",
      "step 1: train loss 129.1268, val loss 129.3054, time elapsed: 5.94 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "torch.save(model.state_dict(),\n",
    "           f'models/{model.__class__.__name__}'\n",
    "           f'-vocab_size{config.vocab_size}'\n",
    "           f'-max_position_embeddings{config.max_position_embeddings}'\n",
    "           f'-num_hidden_layers{config.num_hidden_layers}'\n",
    "           f'-num_attention_heads{config.num_attention_heads}'\n",
    "           f'-num_key_value_heads{config.num_key_value_heads}'\n",
    "           f'-hidden_size{config.hidden_size}'\n",
    "           f'-intermediate_size{config.intermediate_size}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-rms_norm_eps{config.rms_norm_eps}'\n",
    "           f'-rope_theta{config.rope_theta}'\n",
    "           f'-jirachi{config.jirachi}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30e834-56f9-4812-ab4c-03e2f14874ca",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2260c4f-40b2-4b8e-9db4-ad1717a3027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou                                                                                                                                                                                                                      \n"
     ]
    }
   ],
   "source": [
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou \" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "output = model.generate(input_str, output_len = max_useable_output_len)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa447af-c5cd-428c-b830-9e4b05a2c842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
