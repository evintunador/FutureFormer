import torch
import torch.nn as nn


def splice_future_indices(target_tokens, padding_token, mult_factor, max_iter):
    batch_size, max_seq_len = target_tokens.size()
    matrices = []

    length = mult_factor
    tot_length = 1 + length 
    j = 0

    while (tot_length <= max_seq_len) and (j < max_iter):
        matrix = []
        for i in range(max_seq_len):
            subseq = target_tokens[:, i+1:i+1+length]  # slice the target tokens
            
            # If the subsequence is shorter than the required length, pad it with padding_token
            if subseq.size(1) < length:
                padding = torch.full((batch_size, length - subseq.size(1)), padding_token, dtype=torch.long)
                subseq = torch.cat([subseq, padding], dim=1)
            
            matrix.append(subseq)
        
        matrices.append(torch.stack(matrix, dim=1))
        
        length *= mult_factor
        tot_length += length
        j += 1

    return matrices


# Example usage
batch_size = 2
max_seq_len = 15
mult_factor = 2
max_iter = 8
padding_token = 0  # or 'v' if you have a specific padding token
target_tokens = torch.tensor([
    [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],
    [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
])

matrices = splice_future_indices(target_tokens, padding_token, mult_factor, max_iter)

# Print the matrices to verify
for idx, matrix in enumerate(matrices):
    print(f"Matrix {idx+1}:\n{matrix}\n")


# Example usage
batch_size = 32
max_seq_len = 512
mult_factor = 2
max_iter = 6
vocab_size = 8192  
embedding_dim = 128 
target_tokens = torch.randint(vocab_size, (batch_size, max_seq_len))

matrices = splice_future_indices(target_tokens, padding_token, mult_factor, max_iter)

# Print the matrices to verify
for idx, matrix in enumerate(matrices):
    print(f"Matrix {idx+1}: {matrix.shape}")


# Initialize the embedding layer
embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_token)

# Convert matrices to embeddings
embedded_matrices = []
for matrix in matrices:
    embedded_matrix = embedding(matrix)  # Shape: (batch_size, max_seq_len, subseq_length, embedding_dim)
    embedded_matrices.append(embedded_matrix)

# Print the shapes of the embedded matrices to verify
for idx, embedded_matrix in enumerate(embedded_matrices):
    print(f"Embedded Matrix {idx+1} shape: {embedded_matrix.shape}")


import math

class CompressionSchedule:
    def __init__(self, compress_freq, compress_freq_n):
        self.compress_freq = compress_freq
        self.compress_freq_n = compress_freq_n
        
        if compress_freq_n < 1: 
            raise ValueError(f"compress_freq_n must be >= 1")
        if compress_freq == 'log' and compress_freq_n < 2: 
            raise ValueError(f"if using compress_freq=='log' then compress_freq_n must be >= 2")

    def __call__(self, i: int) -> int:
        if self.compress_freq == 'constant':
            return self.constant(i)
        elif self.compress_freq == 'linear':
            return self.linear(i)
        elif self.compress_freq == 'root':
            return self.root(i)
        elif self.compress_freq == 'log':
            return self.log(i)
        elif self.compress_freq == 'poly':
            return self.poly(i)
        else:
            raise ValueError(f"Invalid compression frequency type. {self.compress_freq} is unknkown")

    def constant(self, i: int) -> int:
        return math.floor(self.compress_freq_n)

    def linear(self, i: int) -> int:
        return math.floor(self.compress_freq_n * i + 1)

    def root(self, i: int) -> int:
        return math.floor((i+1) ** (1 / self.compress_freq_n))

    def log(self, i: int) -> int:
        return math.floor(math.log((i+1), self.compress_freq_n))+1

    def poly(self, i: int) -> int:
        return math.floor((i+1) ** self.compress_freq_n)

# Example usage:
scheddy = CompressionSchedule(compress_freq='linear', compress_freq_n=1)

# Forward function
for i in range(16):
    output = scheddy(i)
    print(f"Output for i={i}: {output}")



from modules.pool_operations import *
from modules.norm import Norm


input_norm = Norm(dim=embedding_dim)
module = MaxPooling()
output_norm = Norm(dim=embedding_dim)

# Initially, logging is disabled by default
module.enable_logging()
### Optionally disabling printing for sub-functions
#module.disable_function_logging('')

c = torch.concat(
    [output_norm(
        module(
            input_norm(
                embedded_matrix
            )
        )
    ).unsqueeze(2) for embedded_matrix in embedded_matrices], 
    dim=2
)

print(c.shape)
del input_norm, module, output_norm, c


input_norm = Norm(dim=embedding_dim)
module = SumPooling()
output_norm = Norm(dim=embedding_dim)

c = torch.concat(
    [output_norm(module(input_norm(embedded_matrix))).unsqueeze(2) for embedded_matrix in embedded_matrices], 
    dim=2
)

print(c.shape)
del input_norm, module, output_norm, c


input_norm = Norm(dim=embedding_dim)
module = ParametricSumPooling(embedding_dim, output_seq_len=1, use_output_linear=True)
output_norm = Norm(dim=embedding_dim)

# let's take a look
print(sum(p.numel() for p in module.parameters())/1e3, 'K parameters')
print(module)

# Initially, logging is disabled by default
module.enable_logging()
### Optionally disabling printing for sub-functions
#module.disable_function_logging('')

c = torch.concat(
    [output_norm(module(input_norm(embedded_matrix))) for embedded_matrix in embedded_matrices], 
    dim=2
)

print(c.shape)
del input_norm, module, output_norm, c


# define our compression rate scheduler
scheddy = CompressionSchedule(compress_freq='linear', compress_freq_n=1)

input_norm = Norm(dim=embedding_dim)
output_norm = Norm(dim=embedding_dim)

# Create a list of ParametricMaxPooling modules with different output_seq_len values
modules = [ParametricMaxPooling(embedding_dim, output_seq_len=scheddy(i), use_output_linear=False) for i in range(len(embedded_matrices))]

# Print the total number of parameters in each module
for idx, module in enumerate(modules):
    print(f"Module {idx}: {sum(p.numel() for p in module.parameters()) / 1e3} K parameters")
print(modules[-1])

# Enable logging for each module
for module in modules:
    module.enable_logging()

# Concatenate the outputs from each module
c = torch.concat(
    [module(embedded_matrix) for module, embedded_matrix in zip(modules, embedded_matrices)], 
    dim=2
)

print(c.shape)
del scheddy, input_norm, modules, output_norm, c


# define our compression rate scheduler
scheddy = CompressionSchedule(compress_freq='constant', compress_freq_n=1)

input_norm = Norm(dim=embedding_dim)
output_norm = Norm(dim=embedding_dim)

# Create a list of FlattenProjectionPooling modules with different output_seq_len values
modules = [FlattenProjectionPooling(
    to_be_pooled_seq_len = embedded_matrices[i].shape[2],
    dim = embedding_dim, 
    output_seq_len=scheddy(i)
) for i in range(len(embedded_matrices))]

# Print the total number of parameters in each module
for idx, module in enumerate(modules):
    print(f"Module {idx}: {sum(p.numel() for p in module.parameters()) / 1e3} K parameters")

# Enable logging for each module
for module in modules:
    module.enable_logging()

# Concatenate the outputs from each module
c = torch.concat(
    [output_norm(module(input_norm(embedded_matrix))) for module, embedded_matrix in zip(modules, embedded_matrices)], 
    dim=2
)

print(c.shape)
del scheddy, input_norm, output_norm, module, c


# define our compression rate scheduler
scheddy = CompressionSchedule(compress_freq='root', compress_freq_n=2)

input_norm = Norm(dim=embedding_dim)
output_norm = Norm(dim=embedding_dim)

# Create a list of ConvPooling modules with different output_seq_len values
modules = [ConvPooling(
    to_be_pooled_seq_len = embedded_matrices[i].shape[2],
    dim = embedding_dim, 
    output_seq_len = scheddy(i), 
    use_output_linear=False
) for i in range(len(embedded_matrices))]

# Print the total number of parameters in each module
for idx, module in enumerate(modules):
    print(f"Module {idx}: {sum(p.numel() for p in module.parameters()) / 1e3} K parameters")
print(modules[-1])

# Enable logging for each module
for module in modules:
    module.enable_logging()

# Concatenate the outputs from each module
c = torch.concat(
    [output_norm(module(input_norm(embedded_matrix))) for module, embedded_matrix in zip(modules, embedded_matrices)], 
    dim=2
)

print(c.shape)
del scheddy, input_norm, output_norm, module, c


# define our compression rate scheduler
scheddy = CompressionSchedule(compress_freq='poly', compress_freq_n=1.25)

input_norm = Norm(dim=embedding_dim)
output_norm = Norm(dim=embedding_dim)

# Create a list of AttentionPooling modules with different output_seq_len values
modules = [AttentionPooling(
    dim = embedding_dim, 
    output_seq_len = scheddy(i), 
    use_output_linear=False
) for i in range(len(embedded_matrices))]

# Print the total number of parameters in each module
for idx, module in enumerate(modules):
    print(f"Module {idx}: {sum(p.numel() for p in module.parameters()) / 1e3} K parameters")

# Enable logging for each module
for module in modules:
    module.enable_logging()

# Concatenate the outputs from each module
c = torch.concat(
    [output_norm(module(input_norm(embedded_matrix))) for module, embedded_matrix in zip(modules, embedded_matrices)], 
    dim=2
)

print(c.shape)
del scheddy, input_norm, output_norm, module, c


# define our compression rate scheduler
scheddy = CompressionSchedule(compress_freq='log', compress_freq_n=2)

input_norm = Norm(dim=embedding_dim)
output_norm = Norm(dim=embedding_dim)

# Create a list of SelfAttentionPooling modules with different output_seq_len values
modules = [SelfAttentionPooling(
    dim = embedding_dim, 
    output_seq_len = scheddy(i)
) for i in range(len(embedded_matrices))]

# Print the total number of parameters in each module
for idx, module in enumerate(modules):
    print(f"Module {idx}: {sum(p.numel() for p in module.parameters()) / 1e3} K parameters")

# Enable logging for each module
for module in modules:
    module.enable_logging()

# Concatenate the outputs from each module
c = torch.concat(
    [output_norm(module(input_norm(embedded_matrix))) for module, embedded_matrix in zip(modules, embedded_matrices)], 
    dim=2
)

print(c.shape)
del scheddy, input_norm, output_norm, module, c



